{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d3b412f-0738-4151-a8d7-d9958649f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain openai langchain-community\n",
    "# install the required libraries\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dea0fc-cee1-4977-a168-fec867948901",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- This Python code utilizes Langchain and OpenAI's LLM to create an AI-powered system for processing tweets.\n",
    "- It classifies tweets into categories (question, feedback, complaint, appreciation), analyzes their sentiment (positive, negative, neutral), and generates appropriate responses.\n",
    "- The code employs asynchronous programming (async and await) to handle multiple tweets concurrently, improving efficiency. The nest_asyncio library ensures compatibility with interactive environments like Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f269217-a7dc-4ad2-a933-48eae3bb3f56",
   "metadata": {},
   "source": [
    "# LLM Initialization\n",
    "- This line initializes the Large Language Model (LLM) that will be used for all the subsequent tasks.\n",
    "- OpenAI() creates an instance of the OpenAI LLM. You'll need to have the openai Python library installed (pip install openai) and have your OpenAI API key configured as an environment variable (OPENAI_API_KEY).\n",
    "- temperature=0.7 is a parameter that controls the \"creativity\" of the LLM's output. A lower temperature (closer to 0) makes the output more deterministic and predictable, while a higher temperature (closer to 1) makes the output more creative and unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d95b0a76-016c-44b3-81a2-bb14e0cacc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c9f23-2847-4469-8138-096c74a87603",
   "metadata": {},
   "source": [
    "# Tweet Classifier (TweetClassifier class):\n",
    "- This class is responsible for classifying tweets into predefined categories.\n",
    "- __init__(self, llm): The constructor takes the LLM instance as input and sets up the prompt template and the LLM chain.\n",
    "  - PromptTemplate: Defines the template for the prompt that will be sent to the LLM. The {tweet} placeholder will be replaced with the actual tweet text.\n",
    "  - LLMChain: Combines the LLM and the prompt template to create a chain that can be easily used to generate LLM responses.\n",
    "- async def classify(self, tweet: str) -> str: This is an asynchronous function that takes a tweet as input and returns the predicted category.\n",
    "  - await self.chain.arun(tweet=tweet): This line uses the LLM chain to send the tweet to the LLM for classification. arun is the asynchronous version of run. The await keyword pauses execution until the LLM responds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119c735-4cd0-485f-9b3d-7f3d5e14921c",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer (SentimentAnalyzer class):\n",
    "- This class is very similar to TweetClassifier, but it's responsible for analyzing the sentiment of a tweet (Positive, Negative, or Neutral). It uses a different prompt template tailored for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766b85e-9551-4d0b-ab43-6e7cedadad21",
   "metadata": {},
   "source": [
    "# Response Generator (ResponseGenerator class):\n",
    "- This class generates a response to a tweet, taking into account the tweet itself, its category, and its sentiment. It also uses a different prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930714e-c73a-420f-8ba7-23a1e557bc13",
   "metadata": {},
   "source": [
    "# Twitter Manager (TwitterManager class):\n",
    "- This class acts as an orchestrator. It manages the entire tweet processing workflow.\n",
    "- __init__(self, llm): The constructor takes the LLM and creates instances of the TweetClassifier, SentimentAnalyzer, and ResponseGenerator.\n",
    "- async def handle_tweet(self, tweet: str): This asynchronous function takes a tweet as input and performs the following steps: \n",
    "  - Classifies the tweet using self.classifier.classify(tweet).\n",
    "  - Analyzes the sentiment using self.sentiment_analyzer.analyze(tweet).\n",
    "  - Generates a response using self.response_generator.generate_response(tweet, category, sentiment).\n",
    "  - Returns a dictionary containing the original tweet, its category, sentiment, and the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f2987-301d-46de-ad89-e376ab460609",
   "metadata": {},
   "source": [
    "# Testing the System (main function):\n",
    "- This asynchronous function creates a TwitterManager instance and a list of sample tweets.\n",
    "- It then iterates through the tweets, calling manager.handle_tweet(tweet) for each tweet (using await because handle_tweet is asynchronous).\n",
    "- Finally, it prints the results for each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d06c35-f99b-4a3b-949b-c45a56c18f20",
   "metadata": {},
   "source": [
    "# Running the Main Function:\n",
    "- if __name__ == \"__main__\":: This block ensures that the code inside it only runs when the script is executed directly (not when it's imported as a module).\n",
    "- import asyncio: Imports the asyncio library for asynchronous programming.\n",
    "- import nest_asyncio: Imports the nest_asyncio library. This is crucial for running asynchronous code in environments like Jupyter notebooks, which often have their own running event loops. nest_asyncio allows you to nest event loops.\n",
    "- nest_asyncio.apply(): Applies the nest_asyncio patch, enabling nested event loops.\n",
    "- asyncio.run(main()): Runs the main function using asyncio.run(), which creates and manages the event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0963a6c9-fe71-4056-8b64-3d67b4baafe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: Your product is amazing! Just what I needed ðŸ˜Š\n",
      "Category:  Appreciation\n",
      "Sentiment:  POSITIVE\n",
      "Response:  We're so happy to hear that our product met your needs! Nothing makes us happier than a satisfied customer ðŸ™‚\n",
      "\n",
      "Tweet: How do I reset my password? It's not working ðŸ˜•\n",
      "Category:  Question\n",
      "Sentiment:  NEUTRAL\n",
      "Response:  I'm sorry to hear that you're having trouble logging in. We can definitely help you reset your password. Have you tried using the \"forgot password\" option on the login page? Let me know if you still need assistance! ðŸ™‚\n",
      "\n",
      "Tweet: This product sucks.... ðŸ˜•\n",
      "Category:  Complaint \n",
      "Sentiment:  NEGATIVE\n",
      "Response:  I'm sorry to hear that you're not satisfied with our product! Could you tell me more about what went wrong? We value your feedback and would love to improve your experience with our product.\n",
      "\n",
      "Tweet: Been waiting for support for 2 days now... ðŸ˜ \n",
      "Category:  Complaint\n",
      "Sentiment:  NEGATIVE\n",
      "Response: \n",
      "Hey there! We're sorry to hear that you've been waiting for support for 2 days. We understand that this can be frustrating and we apologize for any inconvenience this may have caused. Can you please DM us your details so we can assist you as soon as possible? We'll make sure to prioritize your issue and get it resolved for you. Thanks for your patience and understanding.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Sorting Agent (Classification)\n",
    "class TweetClassifier:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"tweet\"],\n",
    "            template=\"\"\"\n",
    "            Classify this tweet into one of these categories:\n",
    "            - Question\n",
    "            - Feedback\n",
    "            - Complaint\n",
    "            - Appreciation\n",
    "            \n",
    "            Tweet: {tweet}\n",
    "            Category:\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=llm, prompt=self.prompt)\n",
    "    \n",
    "    async def classify(self, tweet: str) -> str:\n",
    "        return await self.chain.arun(tweet=tweet)\n",
    "\n",
    "# 2. The Mood Detective (Sentiment Analysis)\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"tweet\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the sentiment of this tweet and return either POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "            \n",
    "            Tweet: {tweet}\n",
    "            Sentiment:\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=llm, prompt=self.prompt)\n",
    "    \n",
    "    async def analyze(self, tweet: str) -> str:\n",
    "        return await self.chain.arun(tweet=tweet)\n",
    "\n",
    "# 3. The Reply Writer (Response Generator)\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"tweet\", \"category\", \"sentiment\"],\n",
    "            template=\"\"\"\n",
    "            Create a friendly response to this tweet.\n",
    "            Tweet: {tweet}\n",
    "            Category: {category}\n",
    "            Sentiment: {sentiment}\n",
    "            \n",
    "            Response:\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=llm, prompt=self.prompt)\n",
    "    \n",
    "    async def generate_response(self, tweet: str, category: str, sentiment: str) -> str:\n",
    "        return await self.chain.arun(\n",
    "            tweet=tweet,\n",
    "            category=category,\n",
    "            sentiment=sentiment\n",
    "        )\n",
    "\n",
    "# 4. The Boss Agent (Orchestrator)\n",
    "class TwitterManager:\n",
    "    def __init__(self, llm):\n",
    "        self.classifier = TweetClassifier(llm)\n",
    "        self.sentiment_analyzer = SentimentAnalyzer(llm)\n",
    "        self.response_generator = ResponseGenerator(llm)\n",
    "    \n",
    "    async def handle_tweet(self, tweet: str):\n",
    "        category = await self.classifier.classify(tweet)\n",
    "        sentiment = await self.sentiment_analyzer.analyze(tweet)\n",
    "        response = await self.response_generator.generate_response(tweet, category, sentiment)\n",
    "        \n",
    "        return {\n",
    "            \"tweet\": tweet,\n",
    "            \"category\": category,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "# Testing the System\n",
    "async def main():\n",
    "    manager = TwitterManager(llm)\n",
    "    tweets = [\n",
    "        \"Your product is amazing! Just what I needed ðŸ˜Š\",\n",
    "        \"How do I reset my password? It's not working ðŸ˜•\",\n",
    "        \"This product sucks.... ðŸ˜•\",\n",
    "        \"Been waiting for support for 2 days now... ðŸ˜ \"\n",
    "    ]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        result = await manager.handle_tweet(tweet)\n",
    "        print(\"\\nTweet:\", result[\"tweet\"])\n",
    "        print(\"Category:\", result[\"category\"])\n",
    "        print(\"Sentiment:\", result[\"sentiment\"])\n",
    "        print(\"Response:\", result[\"response\"])\n",
    "\n",
    "# Run the main function\n",
    "import asyncio\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()  # Allows nested event loops\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac2d71-3f14-4aee-b6e4-4af062c7c497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c790979-71a1-404c-8d25-abbec6a4bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3fb85-da23-4041-85a4-21c3a104bde9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3929f-931e-4ca8-b771-846faa119077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learning]",
   "language": "python",
   "name": "conda-env-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
